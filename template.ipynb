{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Title Here\n",
    "\n",
    "**Name(s)**: (your name(s) here)\n",
    "\n",
    "**Website Link**: (your website link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.652554Z",
     "start_time": "2019-10-31T23:36:27.180520Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "pd.options.plotting.backend = 'plotly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "outage_df = pd.read_excel('outage.xlsx')\n",
    "#CLEANING\n",
    "a = dict(zip(list(pd.DataFrame(outage_df.iloc[5:, 1:]).columns), outage_df.iloc[4].tolist()[1:]))\n",
    "outage_cleaned = pd.DataFrame(outage_df.iloc[6:, 1:]).rename(columns=a).reset_index().drop(['index'], axis='columns')\n",
    "outage_cleaned['OUTAGE.START'] = (outage_cleaned['OUTAGE.START.DATE'].transform(lambda x: str(x).split(' ')[0]) + ' ' +  outage_cleaned['OUTAGE.START.TIME'].apply(str)).apply(lambda x: np.NAN if 'nan' in x else pd.to_datetime(x).to_pydatetime())\n",
    "outage_cleaned['OUTAGE.RESTORATION'] = (outage_cleaned['OUTAGE.RESTORATION.DATE'].transform(lambda x: str(x).split(' ')[0]) + ' ' +  outage_cleaned['OUTAGE.RESTORATION.TIME'].apply(str)).apply(lambda x: np.NAN if 'nan' in x else pd.to_datetime(x).to_pydatetime())\n",
    "outage_cleaned = outage_cleaned.drop(['OUTAGE.START.DATE', 'OUTAGE.START.TIME', 'OUTAGE.RESTORATION.DATE', 'OUTAGE.RESTORATION.TIME'], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['severe weather', 'intentional attack', 'system operability disruption',\n",
       "       'public appeal', 'equipment failure', 'fuel supply emergency',\n",
       "       'islanding'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outage_cleaned['CAUSE.CATEGORY'].value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer, KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# woah = outage_cleaned[outage_cleaned['CLIMATE.CATEGORY'].notna() & outage_cleaned['TOTAL.PRICE'].notna()]\n",
    "woah = outage_cleaned.copy()\n",
    "woah = woah.dropna(subset=['CLIMATE.CATEGORY', 'TOTAL.PRICE', 'POPPCT_URBAN', 'OUTAGE.DURATION'])\n",
    "woah['MONTH'] = woah['MONTH'].transform(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepoc = ColumnTransformer([\n",
    "#     # ('imputation', SimpleImputer(strategy='median'), ['CUSTOMERS.AFFECTED']),\n",
    "#     ('climate', OneHotEncoder(handle_unknown='ignore'), ['CLIMATE.CATEGORY']),\n",
    "#     ('test', FunctionTransformer(lambda x: x), ['TOTAL.PRICE']),\n",
    "# ], remainder='drop')\n",
    "\n",
    "# imputer = ColumnTransformer([\n",
    "#     ('imputation', SimpleImputer(strategy='median'), ['CUSTOMERS.AFFECTED']),\n",
    "# ], remainder='passthrough')\n",
    "\n",
    "# pl = Pipeline([\n",
    "#     ('impute', imputer),\n",
    "#     ('prepoc', prepoc),\n",
    "#     ('cook', RandomForestClassifier())\n",
    "# ])\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(woah.drop('MONTH', axis=1), woah['MONTH'], test_size=0.2)\n",
    "\n",
    "# pl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3003412969283277"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pl.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterwong/anaconda3/envs/dsc80/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "1680 fits failed out of a total of 2160.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1680 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/walterwong/anaconda3/envs/dsc80/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/walterwong/anaconda3/envs/dsc80/lib/python3.8/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/walterwong/anaconda3/envs/dsc80/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"/Users/walterwong/anaconda3/envs/dsc80/lib/python3.8/site-packages/joblib/parallel.py\", line 1863, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/Users/walterwong/anaconda3/envs/dsc80/lib/python3.8/site-packages/joblib/parallel.py\", line 1792, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/Users/walterwong/anaconda3/envs/dsc80/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/walterwong/anaconda3/envs/dsc80/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"/Users/walterwong/anaconda3/envs/dsc80/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/walterwong/anaconda3/envs/dsc80/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 308, in fit\n",
      "    raise ValueError(\"max_features must be in (0, n_features]\")\n",
      "ValueError: max_features must be in (0, n_features]\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/walterwong/anaconda3/envs/dsc80/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.48936534 0.4850882  0.48679033 0.48679396 0.4902164  0.4859429\n",
      " 0.49362793 0.48337516 0.41247863 0.42616112 0.42529187 0.42188034\n",
      " 0.4252846  0.41931624 0.4167485  0.42614294 0.38686307 0.38002182\n",
      " 0.38087289 0.3791744  0.37404619 0.38172759 0.38600473 0.37745408\n",
      " 0.36636843 0.35780687 0.36635752 0.35695945 0.35439898 0.3535261\n",
      " 0.35524641 0.35951991 0.34160029 0.34499    0.34499727 0.35268594\n",
      " 0.34328787 0.34756501 0.35012548 0.34158574 0.32792144 0.32878705\n",
      " 0.33561011 0.33305692 0.32963084 0.33305328 0.32536825 0.33475905\n",
      " 0.4859429  0.48849973 0.4867976  0.4936243  0.4902164  0.4936243\n",
      " 0.4876523  0.48850336 0.42358247 0.40821604 0.43296963 0.42185852\n",
      " 0.41847245 0.41930533 0.42613202 0.42101473 0.37660666 0.37917803\n",
      " 0.38429896 0.37233315 0.37659574 0.38428442 0.38173122 0.37915985\n",
      " 0.35098382 0.35951264 0.36122204 0.36378978 0.35610111 0.35182397\n",
      " 0.35695581 0.3543808  0.34415348 0.34755774 0.33902891 0.35098382\n",
      " 0.34841971 0.33987634 0.34670304 0.34073468 0.33818876 0.32536461\n",
      " 0.33904346 0.3398727  0.33134024 0.33218403 0.33304601 0.33218403\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('prepoc',\n",
       "                                        ColumnTransformer(transformers=[('climate',\n",
       "                                                                         OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                                         ['CLIMATE.CATEGORY']),\n",
       "                                                                        ('test',\n",
       "                                                                         FunctionTransformer(func=<function <lambda> at 0x2916c5550>),\n",
       "                                                                         ['TOTAL.PRICE'])])),\n",
       "                                       ('cook', RandomForestClassifier())]),\n",
       "             param_grid={'cook__max_features': [1, 3, 5, 7, 9, 12, 15, 18, 20],\n",
       "                         'cook__min_samples_split': [2, 5, 8, 10, 12, 15],\n",
       "                         'cook__n_estimators': [50, 75, 100, 125, 150, 200, 250,\n",
       "                                                300]})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# params = {\n",
    "#     'cook__n_estimators': [50, 75, 100, 125, 150, 200, 250, 300], \n",
    "#     'cook__max_features': [1, 3, 5, 7, 9, 12, 15, 18, 20], \n",
    "#     'cook__min_samples_split': [2, 5, 8, 10, 12, 15]\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(estimator=pl, param_grid=params, cv=5)\n",
    "# grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cook__max_features': 1,\n",
       " 'cook__min_samples_split': 2,\n",
       " 'cook__n_estimators': 50}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9829205807002562"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pl.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49829351535836175"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pl.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# woah['TOTAL.PRICE'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# woah = outage_cleaned[outage_cleaned['CLIMATE.CATEGORY'].notna() & outage_cleaned['TOTAL.PRICE'].notna()]\n",
    "woah = outage_cleaned.copy()\n",
    "woah = woah.dropna(subset=['CLIMATE.CATEGORY', 'TOTAL.PRICE', 'POPPCT_URBAN', 'OUTAGE.DURATION', 'RES.PRICE', 'TOTAL.SALES', 'TOTAL.CUSTOMERS'])\n",
    "woah['MONTH'] = woah['MONTH'].transform(str)\n",
    "# woah['CUSTOMERS.AFFECTED'] = woah['CUSTOMERS.AFFECTED'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_impute = 'CUSTOMERS.AFFECTED'\n",
    "\n",
    "# Create a KNNImputer object\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Impute missing values in the specified column\n",
    "imputed_data = pd.DataFrame(imputer.fit_transform(woah[[column_to_impute]]))\n",
    "\n",
    "# Update the original DataFrame with the imputed values\n",
    "woah.loc[:, column_to_impute] = imputed_data.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('prepoc',\n",
       "                 ColumnTransformer(transformers=[('climate',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['CLIMATE.CATEGORY']),\n",
       "                                                 ('test',\n",
       "                                                  FunctionTransformer(func=<function <lambda> at 0x2916db160>),\n",
       "                                                  ['TOTAL.PRICE',\n",
       "                                                   'CUSTOMERS.AFFECTED',\n",
       "                                                   'OUTAGE.DURATION',\n",
       "                                                   'RES.PRICE', 'TOTAL.SALES',\n",
       "                                                   'TOTAL.CUSTOMERS'])])),\n",
       "                ('cook', RandomForestClassifier(max_depth=10))])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepoc = ColumnTransformer([\n",
    "    # ('imputation', SimpleImputer(strategy='median'), ['CUSTOMERS.AFFECTED']),\n",
    "    ('climate', OneHotEncoder(handle_unknown='ignore'), ['CLIMATE.CATEGORY']),\n",
    "    # ('log', FunctionTransformer(lambda x: np.log(x + 1)), ['CUSTOMERS.AFFECTED']),\n",
    "    ('test', FunctionTransformer(lambda x: x), ['TOTAL.PRICE', 'CUSTOMERS.AFFECTED', 'OUTAGE.DURATION', 'RES.PRICE', 'TOTAL.SALES', 'TOTAL.CUSTOMERS']),\n",
    "], remainder='drop')\n",
    "\n",
    "# imputer = ColumnTransformer([\n",
    "#     ('imputation', SimpleImputer(strategy='median'), ['CUSTOMERS.AFFECTED']),\n",
    "# ], remainder='passthrough')\n",
    "\n",
    "pl = Pipeline([\n",
    "    # ('impute', imputer),\n",
    "    ('prepoc', prepoc),\n",
    "    ('cook', RandomForestClassifier(max_depth=10))\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(woah.drop('U.S._STATE', axis=1), woah['U.S._STATE'], test_size=0.2)\n",
    "\n",
    "pl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8873720136518771"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9888983774551665"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('prepoc',\n",
       "                 ColumnTransformer(transformers=[('climate',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['CLIMATE.CATEGORY']),\n",
       "                                                 ('log',\n",
       "                                                  FunctionTransformer(func=<function <lambda> at 0x29b426c10>),\n",
       "                                                  ['CUSTOMERS.AFFECTED']),\n",
       "                                                 ('std', StandardScaler(),\n",
       "                                                  ['OUTAGE.DURATION']),\n",
       "                                                 ('square',\n",
       "                                                  FunctionTransformer(func=<function <lambda> at 0x29b426e50>),\n",
       "                                                  ['RES.PRICE']),\n",
       "                                                 ('test',\n",
       "                                                  FunctionTransformer(func=<function <lambda> at 0x29b426ca0>),\n",
       "                                                  ['TOTAL.PRICE',\n",
       "                                                   'CUSTOMERS.AFFECTED',\n",
       "                                                   'OUTAGE.DURATION',\n",
       "                                                   'RES.PRICE', 'TOTAL.SALES',\n",
       "                                                   'TOTAL.CUSTOMERS'])])),\n",
       "                ('cook', RandomForestClassifier())])"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# woah = outage_cleaned[outage_cleaned['CLIMATE.CATEGORY'].notna() & outage_cleaned['TOTAL.PRICE'].notna()]\n",
    "woah = outage_cleaned.copy()\n",
    "woah = woah.dropna(subset=['CLIMATE.CATEGORY', 'TOTAL.PRICE', 'POPPCT_URBAN', 'OUTAGE.DURATION', 'RES.PRICE', 'TOTAL.SALES', 'TOTAL.CUSTOMERS'])\n",
    "woah['MONTH'] = woah['MONTH'].transform(str)\n",
    "# woah['CUSTOMERS.AFFECTED'] = woah['CUSTOMERS.AFFECTED'] + 1\n",
    "column_to_impute = 'CUSTOMERS.AFFECTED'\n",
    "\n",
    "# Create a KNNImputer object\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Impute missing values in the specified column\n",
    "imputed_data = pd.DataFrame(imputer.fit_transform(woah[[column_to_impute]]))\n",
    "\n",
    "# Update the original DataFrame with the imputed values\n",
    "woah.loc[:, column_to_impute] = imputed_data.values.ravel()\n",
    "prepoc = ColumnTransformer([\n",
    "    ('climate', OneHotEncoder(handle_unknown='ignore'), ['CLIMATE.CATEGORY']), #comment out for baseline\n",
    "    ('log', FunctionTransformer(lambda x: np.log(x + 1)), ['CUSTOMERS.AFFECTED']), # comment out for baseline\n",
    "    ('square', FunctionTransformer(lambda x: x**2), ['RES.PRICE']),\n",
    "    ('test', FunctionTransformer(lambda x: x), ['TOTAL.PRICE', 'CUSTOMERS.AFFECTED', 'OUTAGE.DURATION', 'RES.PRICE', 'TOTAL.SALES', 'TOTAL.CUSTOMERS']),\n",
    "    # ('test', FunctionTransformer(lambda x: x), ['TOTAL.PRICE', 'OUTAGE.DURATION']), #baseline\n",
    "], remainder='drop')\n",
    "\n",
    "\n",
    "pl = Pipeline([\n",
    "    ('prepoc', prepoc),\n",
    "    ('cook', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(woah.drop('CAUSE.CATEGORY', axis=1), woah['CAUSE.CATEGORY'], test_size=0.2)\n",
    "\n",
    "pl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7508532423208191"
      ]
     },
     "execution_count": 621,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "south = woah[woah['CLIMATE.REGION'] == 'South']\n",
    "non_south = woah[woah['CLIMATE.REGION'] != 'South']\n",
    "cum_south = 0\n",
    "cum_nonsouth = 0\n",
    "for _ in range(1000):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(non_south.drop('CAUSE.CATEGORY', axis=1), non_south['CAUSE.CATEGORY'], test_size=0.2)\n",
    "    pl.fit(X_train, y_train)\n",
    "    cum_nonsouth += pl.score(X_test, y_test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(south.drop('CAUSE.CATEGORY', axis=1), south['CAUSE.CATEGORY'], test_size=0.2)\n",
    "    pl.fit(X_train, y_train)\n",
    "    cum_south += pl.score(X_test, y_test)\n",
    "observed_diff = (cum_nonsouth - cum_south) / 1000\n",
    "woah_copy = woah.copy()\n",
    "perm_diffs = []\n",
    "for _ in range(1000):\n",
    "    woah_copy['CLIMATE.REGION'] = np.random.permutation(woah_copy['CLIMATE.REGION'])\n",
    "    south = woah_copy[woah_copy['CLIMATE.REGION'] == 'South']\n",
    "    non_south = woah_copy[woah_copy['CLIMATE.REGION'] != 'South']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(non_south.drop('CAUSE.CATEGORY', axis=1), non_south['CAUSE.CATEGORY'], test_size=0.2)\n",
    "    pl.fit(X_train, y_train)\n",
    "    nonsouth_score = pl.score(X_test, y_test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(south.drop('CAUSE.CATEGORY', axis=1), south['CAUSE.CATEGORY'], test_size=0.2)\n",
    "    pl.fit(X_train, y_train)\n",
    "    south_score = pl.score(X_test, y_test)\n",
    "    perm_diffs.append(abs(nonsouth_score-south_score))\n",
    "p_val = (perm_diffs > abs(observed_diff)).mean()\n",
    "# observed_diff\n",
    "# p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8293515358361692"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "woah = outage_cleaned.copy()\n",
    "woah = woah.dropna(subset=['CLIMATE.CATEGORY', 'TOTAL.PRICE', 'POPPCT_URBAN', 'OUTAGE.DURATION', 'RES.PRICE', 'TOTAL.SALES', 'TOTAL.CUSTOMERS'])\n",
    "woah['MONTH'] = woah['MONTH'].transform(str)\n",
    "# woah['CUSTOMERS.AFFECTED'] = woah['CUSTOMERS.AFFECTED'] + 1\n",
    "column_to_impute = 'CUSTOMERS.AFFECTED'\n",
    "\n",
    "# Create a KNNImputer object\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Impute missing values in the specified column\n",
    "imputed_data = pd.DataFrame(imputer.fit_transform(woah[[column_to_impute]]))\n",
    "\n",
    "# Update the original DataFrame with the imputed values\n",
    "woah.loc[:, column_to_impute] = imputed_data.values.ravel()\n",
    "prepoc = ColumnTransformer([\n",
    "    ('climate', OneHotEncoder(handle_unknown='ignore'), ['CLIMATE.CATEGORY']), #comment out for baseline\n",
    "    ('log', FunctionTransformer(lambda x: np.log(x + 1)), ['CUSTOMERS.AFFECTED']), # comment out for baseline\n",
    "    # ('square', FunctionTransformer(lambda x: x**2), ['RES.PRICE']),\n",
    "    ('test', FunctionTransformer(lambda x: x), ['TOTAL.PRICE', 'CUSTOMERS.AFFECTED', 'OUTAGE.DURATION', 'RES.PRICE', 'TOTAL.SALES', 'TOTAL.CUSTOMERS']),\n",
    "    # ('test', FunctionTransformer(lambda x: x), ['TOTAL.PRICE', 'OUTAGE.DURATION']), #baseline\n",
    "], remainder='drop')\n",
    "\n",
    "\n",
    "pl = Pipeline([\n",
    "    ('prepoc', prepoc),\n",
    "    ('cook', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(woah.drop('CAUSE.CATEGORY', axis=1), woah['CAUSE.CATEGORY'], test_size=0.2)\n",
    "\n",
    "pl.fit(X_train, y_train)\n",
    "cum = 0\n",
    "for _ in range(1000):\n",
    "    cum+= pl.score(X_test, y_test)\n",
    "cum/1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framing the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Power outages are a large concern to functioning societies, with massive implications for daily life, commerce, and infrastructure. However, the underlying cause of power outages can vary from natural disasters to equipment failure. To be cognizant of the power outage's cause can provide insight to appropriately respond to each case. For instance, when a power outage results from a xxx, the impact can be devastating with prolonged after effects. On the other hand, outages caused by human error or technical issues often represent a minor inconvenience. \n",
    "\n",
    "By understanding the underlying cause of a power outage, communities can better prepare and respond appropriately, whether it involves implementing robust disaster recovery plans for natural disasters or focusing on regular maintenance and upgrades to mitigate the risk of equipment failures. This awareness enables a more targeted and efficient approach to minimize the impact of power outages on society at large.\n",
    "\n",
    "We will utilize features:\n",
    "\n",
    "\n",
    "We aim to train a classification model using this data that can assist in identifying the CAUSE.CATEGORY of the power outage.....\n",
    "\n",
    "these feature will be preent before identification of the cause....\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "In the original data in the excel file, the 5 rows are unwanted as they are just the title and some notes about the data. The dataframe and its columns \"start\" from the 6th row downward. Additionally, below the columns, there is a row to specify the units of the column if necessary. We also removed this columns when cleaning the data as there are no actual observations.\n",
    "\n",
    "At this point, the data being loaded has all the columns and observations in the excel file data. However, the columns OUTAGE.START.DATE and OUTAGE.START.TIME can be combined as the first always has a time of \"00:00:00\". The same is true for the restoration date and time. We combined these columns into OUTAGE.START and OUTAGE.RESTORATION, dropping the original 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Problem: Classification\n",
    "The prediction problem is to classify the causes of power-outages and we will try to predict all of the power-outage in the U.S.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Variable\n",
    "We will be using multiclassification.\n",
    "The response variable will be the CAUSE.CATEGORY which has these options:\n",
    "- 'severe weather'\n",
    "- 'intentional attack'\n",
    "- 'system operability disruption'\n",
    "- 'public appeal', \n",
    "- 'equipment failure', \n",
    "- 'fuel supply emergency'\n",
    "- 'islanding'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Information\n",
    "\n",
    "We will use Random Forest Classifer in order to predict the CAUSE.CATEGORY. We will use accuracy as it is the most suitable metric in predicting the result since the data set are balanced. Thus it is more suitable than the F-1 score, precision, and recall\n",
    "\n",
    "We will choose features that will be available during the power-outage and not after. The cause of the power-outage is usually the last feature assigned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Description\n",
    "We will try to predict the cause of power-outages based on selected features while feature engineering with one-hot-encoding for the categorical features. \n",
    "The features chosen for this model were 'CLIMATE.CATEGORY', 'TOTAL.PRICE' and 'OUTAGE.DURATION'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features\n",
    "The model uses one-hot-encoding in order to convert the categorical feature 'CAUSE.CATEGORY' into a numerical feature. We held the numerical features as the same.\n",
    "- 'CLIMATE.CATEGORY': A categorical feature that provides information about the climate.\n",
    "- 'TOTAL.PRICE': A quantitative feature that indicates the verage monthly electricity price in the U.S. state (cents/kilowatt-hour)\n",
    "- 'OUTAGE.DURATION': A quantitative feature that indicates the duration of outage events (in minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Performance \n",
    "The model achieved a testing accuracy of 69.62%. Although the relatively low accuracy indicates that our model needs further evaluation and refinement, it shows that we are on the right track. We could improve on the model using more features and feature-engineering by one-hot-encoding categorical features and transforming the numerical features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model\n",
    "### Model Features\n",
    "\n",
    "We choses these features to classify our model:\n",
    "- CLIMATE.CATEGORY: Transformed using one-hot encoding. This feature represents the climate category of the outage and offers insights into the climate region. The climate may indicate the kind of power-outage occured.  \n",
    "\n",
    "- CUSTOMERS.AFFECTED: Log-transformed due to a large cluster of zeroes and large values. This feature represents categories of all the events causing the major power outages. The number of customers affected may be an indicator of how vast the power-outage stretches. \n",
    "\n",
    "- TOTAL.PRICE: This feature represents the average monthly electricity price in the U.S. state (cents/kilowatt-hour). This may be an indicator of how the cost of power-outages can range. \n",
    "\n",
    "- OUTAGE.DURATION: This feature represents the duration of the outage in minutes. Scaling helps handle features with different scales and magnitudes. The duration of the outage may be related to the type of power-outage causing varying lengths of power-outages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance\n",
    "\n",
    "We achieved a testing accuracy of 82.93%. We utilized GridSearchCV in order to find the best hyperparameters. We chose to find number of estimators, maximum features, and minimum sample splits which improved our accuracy slightly. Including more features and feature-enginnering them resulted in a better performing model than the baseline model with accuracy of 69.62%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness Analysis\n",
    "Fairness group choice:\n",
    "Does our model predict worse if the region in the south or elsewhere.\n",
    "\n",
    "Null Hypothesis:\n",
    "Our model has the same prediction accuracy being trained on data from the south vs. data elsewhere.\n",
    "\n",
    "\n",
    "Alternative Hypothesis:\n",
    "Our model has a worse prediction accuracy being trained on data from the south vs. data elsewhere.\n",
    "\n",
    "\n",
    "Test Statistic:\n",
    "Difference in accuracy\n",
    "\n",
    "Signficant level:\n",
    "0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Test\n",
    "We found the observed difference of accuracy between data from the south and the data elsewhere. Then we shuffled the CLIMATE.REGION column to see if there is a change in accuracy. Then we find the permuted difference of accuracy between data from the data from the south and data elsewhere. After permutation testing, we resulted in a p-value of 0.187. This is higher than our significant level of 0.01. Therefore, we fail to reject the null hypothesis and claim that our model predicts the same being trained on data from the south and data elsewhere.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.18 ('dsc80')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "792802a59b927c6a9c7f9d30a3a30b8d00b51b910961ce625c5caa31a5bd4aff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
