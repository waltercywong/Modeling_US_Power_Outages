{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Title Here\n",
    "\n",
    "**Name(s)**: (your name(s) here)\n",
    "\n",
    "**Website Link**: (your website link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.652554Z",
     "start_time": "2019-10-31T23:36:27.180520Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "pd.options.plotting.backend = 'plotly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "outage_df = pd.read_excel('outage.xlsx')\n",
    "#CLEANING\n",
    "a = dict(zip(list(pd.DataFrame(outage_df.iloc[5:, 1:]).columns), outage_df.iloc[4].tolist()[1:]))\n",
    "outage_cleaned = pd.DataFrame(outage_df.iloc[6:, 1:]).rename(columns=a).reset_index().drop(['index'], axis='columns')\n",
    "outage_cleaned['OUTAGE.START'] = (outage_cleaned['OUTAGE.START.DATE'].transform(lambda x: str(x).split(' ')[0]) + ' ' +  outage_cleaned['OUTAGE.START.TIME'].apply(str)).apply(lambda x: np.NAN if 'nan' in x else pd.to_datetime(x).to_pydatetime())\n",
    "outage_cleaned['OUTAGE.RESTORATION'] = (outage_cleaned['OUTAGE.RESTORATION.DATE'].transform(lambda x: str(x).split(' ')[0]) + ' ' +  outage_cleaned['OUTAGE.RESTORATION.TIME'].apply(str)).apply(lambda x: np.NAN if 'nan' in x else pd.to_datetime(x).to_pydatetime())\n",
    "outage_cleaned = outage_cleaned.drop(['OUTAGE.START.DATE', 'OUTAGE.START.TIME', 'OUTAGE.RESTORATION.DATE', 'OUTAGE.RESTORATION.TIME'], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['severe weather', 'intentional attack', 'system operability disruption',\n",
       "       'public appeal', 'equipment failure', 'fuel supply emergency',\n",
       "       'islanding'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outage_cleaned['CAUSE.CATEGORY'].value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer, KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# woah = outage_cleaned[outage_cleaned['CLIMATE.CATEGORY'].notna() & outage_cleaned['TOTAL.PRICE'].notna()]\n",
    "woah = outage_cleaned.copy()\n",
    "woah = woah.dropna(subset=['CLIMATE.CATEGORY', 'TOTAL.PRICE', 'POPPCT_URBAN', 'OUTAGE.DURATION'])\n",
    "woah['MONTH'] = woah['MONTH'].transform(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Specifying the columns using strings is only supported for pandas DataFrames",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/sklearn/utils/__init__.py:409\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m     all_columns \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39;49mcolumns\n\u001b[1;32m    410\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m\n\u001b[1;32m     11\u001b[0m pl \u001b[39m=\u001b[39m Pipeline([\n\u001b[1;32m     12\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mimpute\u001b[39m\u001b[39m'\u001b[39m, imputer),\n\u001b[1;32m     13\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mprepoc\u001b[39m\u001b[39m'\u001b[39m, prepoc),\n\u001b[1;32m     14\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mcook\u001b[39m\u001b[39m'\u001b[39m, RandomForestClassifier())\n\u001b[1;32m     15\u001b[0m ])\n\u001b[1;32m     17\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(woah\u001b[39m.\u001b[39mdrop(\u001b[39m'\u001b[39m\u001b[39mMONTH\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), woah[\u001b[39m'\u001b[39m\u001b[39mMONTH\u001b[39m\u001b[39m'\u001b[39m], test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m pl\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/sklearn/pipeline.py:390\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \n\u001b[1;32m    366\u001b[0m \u001b[39mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    389\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m--> 390\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps)\n\u001b[1;32m    391\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[1;32m    392\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/sklearn/pipeline.py:348\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    346\u001b[0m     cloned_transformer \u001b[39m=\u001b[39m clone(transformer)\n\u001b[1;32m    347\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[1;32m    349\u001b[0m     cloned_transformer,\n\u001b[1;32m    350\u001b[0m     X,\n\u001b[1;32m    351\u001b[0m     y,\n\u001b[1;32m    352\u001b[0m     \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    353\u001b[0m     message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPipeline\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    354\u001b[0m     message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(step_idx),\n\u001b[1;32m    355\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps[name],\n\u001b[1;32m    356\u001b[0m )\n\u001b[1;32m    357\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/joblib/memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 353\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/sklearn/pipeline.py:893\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    892\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 893\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mfit_transform(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    894\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    895\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py:672\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_n_features(X, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    671\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_transformers()\n\u001b[0;32m--> 672\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_column_callables(X)\n\u001b[1;32m    673\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_remainder(X)\n\u001b[1;32m    675\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_transform(X, y, _fit_transform_one)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py:352\u001b[0m, in \u001b[0;36mColumnTransformer._validate_column_callables\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    350\u001b[0m         columns \u001b[39m=\u001b[39m columns(X)\n\u001b[1;32m    351\u001b[0m     all_columns\u001b[39m.\u001b[39mappend(columns)\n\u001b[0;32m--> 352\u001b[0m     transformer_to_input_indices[name] \u001b[39m=\u001b[39m _get_column_indices(X, columns)\n\u001b[1;32m    354\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_columns \u001b[39m=\u001b[39m all_columns\n\u001b[1;32m    355\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transformer_to_input_indices \u001b[39m=\u001b[39m transformer_to_input_indices\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/sklearn/utils/__init__.py:411\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    409\u001b[0m     all_columns \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m    410\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m--> 411\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    412\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSpecifying the columns using strings is only \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    413\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msupported for pandas DataFrames\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    414\u001b[0m     )\n\u001b[1;32m    415\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    416\u001b[0m     columns \u001b[39m=\u001b[39m [key]\n",
      "\u001b[0;31mValueError\u001b[0m: Specifying the columns using strings is only supported for pandas DataFrames"
     ]
    }
   ],
   "source": [
    "prepoc = ColumnTransformer([\n",
    "    # ('imputation', SimpleImputer(strategy='median'), ['CUSTOMERS.AFFECTED']),\n",
    "    ('climate', OneHotEncoder(handle_unknown='ignore'), ['CLIMATE.CATEGORY']),\n",
    "    ('test', FunctionTransformer(lambda x: x), ['TOTAL.PRICE']),\n",
    "], remainder='drop')\n",
    "\n",
    "imputer = ColumnTransformer([\n",
    "    ('imputation', SimpleImputer(strategy='median'), ['CUSTOMERS.AFFECTED']),\n",
    "], remainder='passthrough')\n",
    "\n",
    "pl = Pipeline([\n",
    "    ('impute', imputer),\n",
    "    ('prepoc', prepoc),\n",
    "    ('cook', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(woah.drop('MONTH', axis=1), woah['MONTH'], test_size=0.2)\n",
    "\n",
    "pl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3003412969283277"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterwong/anaconda3/envs/dsc80/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "1680 fits failed out of a total of 2160.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1680 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/walterwong/anaconda3/envs/dsc80/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/walterwong/anaconda3/envs/dsc80/lib/python3.8/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/walterwong/anaconda3/envs/dsc80/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"/Users/walterwong/anaconda3/envs/dsc80/lib/python3.8/site-packages/joblib/parallel.py\", line 1863, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/Users/walterwong/anaconda3/envs/dsc80/lib/python3.8/site-packages/joblib/parallel.py\", line 1792, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/Users/walterwong/anaconda3/envs/dsc80/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/walterwong/anaconda3/envs/dsc80/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"/Users/walterwong/anaconda3/envs/dsc80/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/walterwong/anaconda3/envs/dsc80/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 308, in fit\n",
      "    raise ValueError(\"max_features must be in (0, n_features]\")\n",
      "ValueError: max_features must be in (0, n_features]\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/walterwong/anaconda3/envs/dsc80/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.48936534 0.4850882  0.48679033 0.48679396 0.4902164  0.4859429\n",
      " 0.49362793 0.48337516 0.41247863 0.42616112 0.42529187 0.42188034\n",
      " 0.4252846  0.41931624 0.4167485  0.42614294 0.38686307 0.38002182\n",
      " 0.38087289 0.3791744  0.37404619 0.38172759 0.38600473 0.37745408\n",
      " 0.36636843 0.35780687 0.36635752 0.35695945 0.35439898 0.3535261\n",
      " 0.35524641 0.35951991 0.34160029 0.34499    0.34499727 0.35268594\n",
      " 0.34328787 0.34756501 0.35012548 0.34158574 0.32792144 0.32878705\n",
      " 0.33561011 0.33305692 0.32963084 0.33305328 0.32536825 0.33475905\n",
      " 0.4859429  0.48849973 0.4867976  0.4936243  0.4902164  0.4936243\n",
      " 0.4876523  0.48850336 0.42358247 0.40821604 0.43296963 0.42185852\n",
      " 0.41847245 0.41930533 0.42613202 0.42101473 0.37660666 0.37917803\n",
      " 0.38429896 0.37233315 0.37659574 0.38428442 0.38173122 0.37915985\n",
      " 0.35098382 0.35951264 0.36122204 0.36378978 0.35610111 0.35182397\n",
      " 0.35695581 0.3543808  0.34415348 0.34755774 0.33902891 0.35098382\n",
      " 0.34841971 0.33987634 0.34670304 0.34073468 0.33818876 0.32536461\n",
      " 0.33904346 0.3398727  0.33134024 0.33218403 0.33304601 0.33218403\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('prepoc',\n",
       "                                        ColumnTransformer(transformers=[('climate',\n",
       "                                                                         OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                                         ['CLIMATE.CATEGORY']),\n",
       "                                                                        ('test',\n",
       "                                                                         FunctionTransformer(func=<function <lambda> at 0x2916c5550>),\n",
       "                                                                         ['TOTAL.PRICE'])])),\n",
       "                                       ('cook', RandomForestClassifier())]),\n",
       "             param_grid={'cook__max_features': [1, 3, 5, 7, 9, 12, 15, 18, 20],\n",
       "                         'cook__min_samples_split': [2, 5, 8, 10, 12, 15],\n",
       "                         'cook__n_estimators': [50, 75, 100, 125, 150, 200, 250,\n",
       "                                                300]})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'cook__n_estimators': [50, 75, 100, 125, 150, 200, 250, 300], \n",
    "    'cook__max_features': [1, 3, 5, 7, 9, 12, 15, 18, 20], \n",
    "    'cook__min_samples_split': [2, 5, 8, 10, 12, 15]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pl, param_grid=params, cv=5)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cook__max_features': 1,\n",
       " 'cook__min_samples_split': 2,\n",
       " 'cook__n_estimators': 50}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9829205807002562"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49829351535836175"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "woah['TOTAL.PRICE'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# woah = outage_cleaned[outage_cleaned['CLIMATE.CATEGORY'].notna() & outage_cleaned['TOTAL.PRICE'].notna()]\n",
    "woah = outage_cleaned.copy()\n",
    "woah = woah.dropna(subset=['CLIMATE.CATEGORY', 'TOTAL.PRICE', 'POPPCT_URBAN', 'OUTAGE.DURATION', 'RES.PRICE', 'TOTAL.SALES', 'TOTAL.CUSTOMERS'])\n",
    "woah['MONTH'] = woah['MONTH'].transform(str)\n",
    "# woah['CUSTOMERS.AFFECTED'] = woah['CUSTOMERS.AFFECTED'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_impute = 'CUSTOMERS.AFFECTED'\n",
    "\n",
    "# Create a KNNImputer object\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Impute missing values in the specified column\n",
    "imputed_data = pd.DataFrame(imputer.fit_transform(woah[[column_to_impute]]))\n",
    "\n",
    "# Update the original DataFrame with the imputed values\n",
    "woah.loc[:, column_to_impute] = imputed_data.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('prepoc',\n",
       "                 ColumnTransformer(transformers=[('climate',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['CLIMATE.CATEGORY']),\n",
       "                                                 ('test',\n",
       "                                                  FunctionTransformer(func=<function <lambda> at 0x2916db160>),\n",
       "                                                  ['TOTAL.PRICE',\n",
       "                                                   'CUSTOMERS.AFFECTED',\n",
       "                                                   'OUTAGE.DURATION',\n",
       "                                                   'RES.PRICE', 'TOTAL.SALES',\n",
       "                                                   'TOTAL.CUSTOMERS'])])),\n",
       "                ('cook', RandomForestClassifier(max_depth=10))])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepoc = ColumnTransformer([\n",
    "    # ('imputation', SimpleImputer(strategy='median'), ['CUSTOMERS.AFFECTED']),\n",
    "    ('climate', OneHotEncoder(handle_unknown='ignore'), ['CLIMATE.CATEGORY']),\n",
    "    # ('log', FunctionTransformer(lambda x: np.log(x + 1)), ['CUSTOMERS.AFFECTED']),\n",
    "    ('test', FunctionTransformer(lambda x: x), ['TOTAL.PRICE', 'CUSTOMERS.AFFECTED', 'OUTAGE.DURATION', 'RES.PRICE', 'TOTAL.SALES', 'TOTAL.CUSTOMERS']),\n",
    "], remainder='drop')\n",
    "\n",
    "# imputer = ColumnTransformer([\n",
    "#     ('imputation', SimpleImputer(strategy='median'), ['CUSTOMERS.AFFECTED']),\n",
    "# ], remainder='passthrough')\n",
    "\n",
    "pl = Pipeline([\n",
    "    # ('impute', imputer),\n",
    "    ('prepoc', prepoc),\n",
    "    ('cook', RandomForestClassifier(max_depth=10))\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(woah.drop('U.S._STATE', axis=1), woah['U.S._STATE'], test_size=0.2)\n",
    "\n",
    "pl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8873720136518771"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9888983774551665"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('prepoc',\n",
       "                 ColumnTransformer(transformers=[('climate',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['CLIMATE.CATEGORY']),\n",
       "                                                 ('log',\n",
       "                                                  FunctionTransformer(func=<function <lambda> at 0x29b426c10>),\n",
       "                                                  ['CUSTOMERS.AFFECTED']),\n",
       "                                                 ('std', StandardScaler(),\n",
       "                                                  ['OUTAGE.DURATION']),\n",
       "                                                 ('square',\n",
       "                                                  FunctionTransformer(func=<function <lambda> at 0x29b426e50>),\n",
       "                                                  ['RES.PRICE']),\n",
       "                                                 ('test',\n",
       "                                                  FunctionTransformer(func=<function <lambda> at 0x29b426ca0>),\n",
       "                                                  ['TOTAL.PRICE',\n",
       "                                                   'CUSTOMERS.AFFECTED',\n",
       "                                                   'OUTAGE.DURATION',\n",
       "                                                   'RES.PRICE', 'TOTAL.SALES',\n",
       "                                                   'TOTAL.CUSTOMERS'])])),\n",
       "                ('cook', RandomForestClassifier())])"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# woah = outage_cleaned[outage_cleaned['CLIMATE.CATEGORY'].notna() & outage_cleaned['TOTAL.PRICE'].notna()]\n",
    "woah = outage_cleaned.copy()\n",
    "woah = woah.dropna(subset=['CLIMATE.CATEGORY', 'TOTAL.PRICE', 'POPPCT_URBAN', 'OUTAGE.DURATION', 'RES.PRICE', 'TOTAL.SALES', 'TOTAL.CUSTOMERS'])\n",
    "woah['MONTH'] = woah['MONTH'].transform(str)\n",
    "# woah['CUSTOMERS.AFFECTED'] = woah['CUSTOMERS.AFFECTED'] + 1\n",
    "column_to_impute = 'CUSTOMERS.AFFECTED'\n",
    "\n",
    "# Create a KNNImputer object\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Impute missing values in the specified column\n",
    "imputed_data = pd.DataFrame(imputer.fit_transform(woah[[column_to_impute]]))\n",
    "\n",
    "# Update the original DataFrame with the imputed values\n",
    "woah.loc[:, column_to_impute] = imputed_data.values.ravel()\n",
    "prepoc = ColumnTransformer([\n",
    "    ('climate', OneHotEncoder(handle_unknown='ignore'), ['CLIMATE.CATEGORY']), #comment out for baseline\n",
    "    ('log', FunctionTransformer(lambda x: np.log(x + 1)), ['CUSTOMERS.AFFECTED']), # comment out for baseline\n",
    "    ('square', FunctionTransformer(lambda x: x**2), ['RES.PRICE']),\n",
    "    ('test', FunctionTransformer(lambda x: x), ['TOTAL.PRICE', 'CUSTOMERS.AFFECTED', 'OUTAGE.DURATION', 'RES.PRICE', 'TOTAL.SALES', 'TOTAL.CUSTOMERS']),\n",
    "    # ('test', FunctionTransformer(lambda x: x), ['TOTAL.PRICE', 'OUTAGE.DURATION']), #baseline\n",
    "], remainder='drop')\n",
    "\n",
    "\n",
    "pl = Pipeline([\n",
    "    ('prepoc', prepoc),\n",
    "    ('cook', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(woah.drop('CAUSE.CATEGORY', axis=1), woah['CAUSE.CATEGORY'], test_size=0.2)\n",
    "\n",
    "pl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7508532423208191"
      ]
     },
     "execution_count": 621,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framing the Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.657068Z",
     "start_time": "2019-10-31T23:36:28.654650Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Power outages are a large concern to functioning societies, with massive implications for daily life, commerce, and infrastructure. However, the underlying cause of power outages can vary from natural disasters to equipment failure. To be cognizant of the power outage's cause can provide insight to appropriately respond to each case. For instance, when a power outage results from a xxx, the impact can be devastating with prolonged after effects. On the other hand, outages caused by human error or technical issues often represent a minor inconvenience. \n",
    "\n",
    "By understanding the underlying cause of a power outage, communities can better prepare and respond appropriately, whether it involves implementing robust disaster recovery plans for natural disasters or focusing on regular maintenance and upgrades to mitigate the risk of equipment failures. This awareness enables a more targeted and efficient approach to minimize the impact of power outages on society at large.\n",
    "\n",
    "We will utilize features:\n",
    "\n",
    "\n",
    "We aim to train a classification model using this data that can assist in identifying the CAUSE.CATEGORY of the power outage.....\n",
    "\n",
    "these feature will be preent before identification of the cause....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "In the original data in the excel file, the 5 rows are unwanted as they are just the title and some notes about the data. The dataframe and its columns \"start\" from the 6th row downward. Additionally, below the columns, there is a row to specify the units of the column if necessary. We also removed this columns when cleaning the data as there are no actual observations.\n",
    "\n",
    "At this point, the data being loaded has all the columns and observations in the excel file data. However, the columns OUTAGE.START.DATE and OUTAGE.START.TIME can be combined as the first always has a time of \"00:00:00\". The same is true for the restoration date and time. We combined these columns into OUTAGE.START and OUTAGE.RESTORATION, dropping the original 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Variable\n",
    "The response variable will be the CAUSE.CATEGORY which has these options:\n",
    "- 'severe weather'\n",
    "- 'intentional attack'\n",
    "- 'system operability disruption'\n",
    "- 'public appeal', \n",
    "- 'equipment failure', \n",
    "- 'fuel supply emergency'\n",
    "- 'islanding'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Justification\n",
    "blah blah blah\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "To train the model, we will use ...... These columns will provide relevant information for predicting the power outage’s location. At the time of prediction, we would have access to the features mentioned above for the power outage in question. We would not have any future information beyond what is available in the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric for Evaluation\n",
    "we will use chi-squared\n",
    "\n",
    "## Justification for Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.666489Z",
     "start_time": "2019-10-31T23:36:28.664381Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.18 ('dsc80')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "792802a59b927c6a9c7f9d30a3a30b8d00b51b910961ce625c5caa31a5bd4aff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
